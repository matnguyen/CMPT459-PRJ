{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.metrics import accuracy_score, auc, roc_curve\n",
    "\n",
    "# Decision tree specific modules\n",
    "from sklearn import tree\n",
    "\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer #Can use tfidffvectorizer as well\n",
    "import pandas as pd \n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = os.path.join(os.getcwd(), os.pardir)\n",
    "DATA_PATH = os.path.join(BASE_PATH, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = load_npz(os.path.join(DATA_PATH, 'training_feats.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = load_npz(os.path.join(DATA_PATH, 'test_feats.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49308, 35520)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74659, 35520)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_json(os.path.join(BASE_PATH, '01-milestone1', 'imputed_train.json'))\n",
    "# test_df =  pd.read_json(os.path.join(DATA_PATH, 'test.json.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.DataFrame(train_df)\n",
    "# test_df =  pd.DataFrame(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = train_df.drop(columns=['interest_level'])\n",
    "y_train = train_df['interest_level']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "As described in milestone 1, we will do some feature extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(train_df, min_feats=5):\n",
    "    bathrooms = train_df['bathrooms']\n",
    "    bedrooms = train_df['bedrooms']\n",
    "    building_ids = train_df['building_id']\n",
    "    latitudes = train_df['latitude']\n",
    "    longitudes = train_df['longitude']\n",
    "    manager_ids = train_df['manager_id']\n",
    "    prices = train_df['price']\n",
    "    \n",
    "    datetime = pd.to_datetime(train_df['created'])\n",
    "    \n",
    "    months = datetime.dt.month\n",
    "    days = datetime.dt.day\n",
    "    hours = datetime.dt.hour\n",
    "    \n",
    "    # Where Monday = 0, and Sunday = 6\n",
    "    weekdays = datetime.dt.dayofweek\n",
    "    \n",
    "    num_photos = train_df['photos'].str.len()\n",
    "    \n",
    "    features = train_df['features'].apply(lambda x: [i.lower() for i in x])\n",
    "\n",
    "    feature_counts = Counter()\n",
    "    for feature in features.tolist():\n",
    "        feature_counts.update(feature)\n",
    "    feature = sorted([k for (k,v) in feature_counts.items() if v > min_feats])\n",
    "    \n",
    "    key2original = defaultdict(list)\n",
    "    k = 4\n",
    "    for f in feature:\n",
    "        cleaned = clean(f)\n",
    "        key = cleaned[:k].strip()\n",
    "        key2original[key].append(f)\n",
    "    \n",
    "    columns = list(key2original.keys())\n",
    "    \n",
    "    # reverse hash table\n",
    "    original2key = {}\n",
    "    for col in columns:\n",
    "        for original in key2original[col]:\n",
    "            original2key[original] = col\n",
    "            \n",
    "    all_listing_features = {}\n",
    "\n",
    "    for index,row in train_df.iterrows():\n",
    "        listing_features = {}\n",
    "        features_found = []\n",
    "        for feature in row['features']:\n",
    "            feature = feature.lower()\n",
    "            if feature in original2key:\n",
    "                features_found.append(original2key[feature])\n",
    "        for feature in columns:\n",
    "                if feature not in features_found:\n",
    "                    listing_features[feature] = 0\n",
    "                else:\n",
    "                    listing_features[feature] = 1\n",
    "        all_listing_features[row['listing_id']] = listing_features\n",
    "\n",
    "    one_hot_features = pd.DataFrame.from_dict(all_listing_features, orient='index')\n",
    "    \n",
    "    # Description attribute\n",
    "    \n",
    "    descriptions = train_df[['description']]\n",
    "    # Removes symbols, numbers and stem the words to reduce dimentional space\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    descriptions['description_new'] = descriptions.description.apply(lambda x: clean_description(x, stemmer))\n",
    "\n",
    "    cvect_desc = CountVectorizer(stop_words='english', max_features=200)\n",
    "    full_sparse = cvect_desc.fit_transform(descriptions.description_new)\n",
    "\n",
    "    # Renaming words to avoid collisions with other feature names in the model\n",
    "    col_desc = ['desc_'+ i for i in cvect_desc.get_feature_names()] \n",
    "    count_vect_df = pd.DataFrame(full_sparse.todense(), columns=col_desc)\n",
    "    descriptions = pd.concat([descriptions.reset_index(), count_vect_df],axis=1)\n",
    "    \n",
    "    descriptions = descriptions.drop(labels=['description', 'index', 'description_new'], axis=1)\n",
    "    descriptions.index = train_df['listing_id']\n",
    "    \n",
    "    # The final dataframe to be returned\n",
    "    final_train_df = pd.DataFrame()\n",
    "    \n",
    "    final_train_df['bathrooms'] = bathrooms\n",
    "    final_train_df['bedrooms'] = bedrooms\n",
    "    final_train_df['building_ids'] = building_ids \n",
    "    final_train_df['latitudes'] = latitudes\n",
    "    final_train_df['longitudes'] = longitudes\n",
    "    final_train_df['manager_ids'] = manager_ids\n",
    "    final_train_df['prices'] = prices\n",
    "    final_train_df['months'] = months\n",
    "    final_train_df['days'] = days\n",
    "    final_train_df['hours'] = hours\n",
    "    final_train_df['weekdays'] = weekdays\n",
    "    final_train_df['num_photos'] = num_photos\n",
    "    final_train_df.index = train_df['listing_id']\n",
    "    \n",
    "    final_train_df = final_train_df.merge(descriptions, left_index=True, right_index=True)\n",
    "    final_train_df = final_train_df.merge(one_hot_features, left_index=True, right_index=True)\n",
    "    \n",
    "    final_train_df = pd.concat([final_train_df, pd.get_dummies(final_train_df['building_ids'], prefix='building')], axis=1)\n",
    "    final_train_df = pd.concat([final_train_df, pd.get_dummies(final_train_df['manager_ids'], prefix='manager')], axis=1)\n",
    "    final_train_df = final_train_df.drop(['building_ids', 'manager_ids'], axis=1)\n",
    "\n",
    "    return final_train_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(s):\n",
    "    x = s.replace(\"-\", \"\")\n",
    "    x = x.replace(\" \", \"\")\n",
    "    x = x.replace(\"twenty four hour\", \"24\")\n",
    "    x = x.replace(\"24/7\", \"24\")\n",
    "    x = x.replace(\"24hr\", \"24\")\n",
    "    x = x.replace(\"24-hour\", \"24\")\n",
    "    x = x.replace(\"24hour\", \"24\")\n",
    "    x = x.replace(\"24 hour\", \"24\")\n",
    "    x = x.replace(\"common\", \"cm\")\n",
    "    x = x.replace(\"concierge\", \"doorman\")\n",
    "    x = x.replace(\"bicycle\", \"bike\")\n",
    "    x = x.replace(\"private\", \"pv\")\n",
    "    x = x.replace(\"deco\", \"dc\")\n",
    "    x = x.replace(\"decorative\", \"dc\")\n",
    "    x = x.replace(\"onsite\", \"os\")\n",
    "    x = x.replace(\"outdoor\", \"od\")\n",
    "    x = x.replace(\"ss appliances\", \"stainless\")\n",
    "    return x\n",
    "\n",
    "def clean_description(x, stemmer):\n",
    "    regex = re.compile('[^a-zA-Z ]')\n",
    "    # For user clarity, broken it into three steps\n",
    "    i = regex.sub(' ', x).lower()\n",
    "    i = i.split(\" \") \n",
    "    i= [stemmer.stem(l) for l in i]\n",
    "    i= \" \".join([l.strip() for l in i if (len(l)>2) ]) # Keeping words that have length greater than 2\n",
    "    return i\n",
    "\n",
    "def feature_hash(x):\n",
    "    cleaned = clean(x, uniq)\n",
    "    key = cleaned[:4].strip()\n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\L\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\L\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# X_train = preprocess(X_train)\n",
    "# X_test = preprocess(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For submission purposes, we need to store the listing_id of X_test\n",
    "# X_test_listing_ids = X_test.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "We first instantiate a decision tree model and train it naively using all the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels from {low, medium high} -> {0, 1, 2}\n",
    "le = preprocessing.LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split\n",
    "\n",
    "We need to split the training data into a training set and a validation set (to evaluate the performance of the model).\n",
    "\n",
    "For now we do 5-fold cross_validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 36201431"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_info = mutual_info_classif(X_train, y_train, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1000_features = mutual_info.argsort()[-1000:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49308, 35520)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74659, 35520)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use these top 1000 features when training, to eliminate overfitting.\n",
    "X_train_final = X_train[:,top_1000_features]\n",
    "X_test_final = X_test[:,top_1000_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss score using gini as the criterion:\n",
      "11.6219\n",
      "Log loss score using entropy as the criterion:\n",
      "11.6409\n"
     ]
    }
   ],
   "source": [
    "criterions = ['gini', 'entropy']\n",
    "for criterion in criterions:\n",
    "    model = tree.DecisionTreeClassifier(criterion=criterion)\n",
    "    scores = cross_validate(model, X_train_final, y_train, scoring='neg_log_loss', cv=5)\n",
    "    # Convert negative log loss to log loss\n",
    "    test_scores = -1 * scores['test_score']\n",
    "    print(\"Log loss score using {0} as the criterion:\".format(criterion))\n",
    "    print(test_scores.mean().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try training on the entire train set. We will obtain a model, train it on the test dataset, then submit to kaggle for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tree.DecisionTreeClassifier(criterion='gini')\n",
    "model.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_proba(X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['high', 'low', 'medium'], dtype=object)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the order is not correct (low corresponds to the label 1, but in the csv they expect the low to be the third column), we must swap columns for y_pred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission_csv(X_test, y_pred):\n",
    "    df = pd.DataFrame(y_pred, columns=le.classes_)\n",
    "    df.index = X_test.index\n",
    "    df.index.name = 'listing_id'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = create_submission_csv(X_test, y_pred)\n",
    "output.to_csv('decision_tree_predictions_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score from Kaggle\n",
    "9.18820 <- ignore, this is the old score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some additional things to try:\n",
    "\n",
    "- entropy for information gain\n",
    "- limiting max_depth to like 30 or so\n",
    "- class_weight = balanced\n",
    "- ccp_alpha = ??? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
