{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, auc, roc_curve\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
    "\n",
    "# Decision tree specific modules\n",
    "from sklearn import tree\n",
    "\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer #Can use tfidffvectorizer as well\n",
    "import pandas as pd \n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = os.path.join(os.getcwd(), os.pardir)\n",
    "DATA_PATH = os.path.join(BASE_PATH, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = load_npz(os.path.join(DATA_PATH, 'training_feats.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = load_npz(os.path.join(DATA_PATH, 'test_feats.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49308, 35522)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74659, 35522)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_json(os.path.join(BASE_PATH, '01-milestone1', 'imputed_train.json'))\n",
    "test_df =  pd.read_json(os.path.join(DATA_PATH, 'test.json.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df['interest_level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_indexes = test_df.listing_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "As described in milestone 1, we will do some feature extraction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "We first instantiate a decision tree model and train it naively using all the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels from {low, medium high} -> {0, 1, 2}\n",
    "le = preprocessing.LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split\n",
    "\n",
    "We need to split the training data into a training set and a validation set (to evaluate the performance of the model).\n",
    "\n",
    "For now we do 5-fold cross_validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 36201431"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial approach is to use all features for training. For the parameters, we simply use the defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_feat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-6e7a87329bfc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mtrain_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Mean log loss test  score: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_feat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Mean log loss train score: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_feat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n_feat' is not defined"
     ]
    }
   ],
   "source": [
    "model = tree.DecisionTreeClassifier()\n",
    "scores = cross_validate(model, X_train, y_train, scoring='neg_log_loss', cv=5, return_train_score=True)\n",
    "# Convert negative log loss to log loss\n",
    "test_scores = -1 * scores['test_score']\n",
    "train_scores = -1 * scores['train_score']\n",
    "\n",
    "score = test_scores.mean().round(4)\n",
    "train_score = train_scores.mean().round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean log loss test  score: 11.19\n",
      "Mean log loss train score: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Mean log loss test  score: {0}\".format(score))\n",
    "print(\"Mean log loss train score: {0}\".format(train_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try training on the entire train set. We will obtain a model, train it on the test dataset, then submit to kaggle for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tree.DecisionTreeClassifier(criterion='gini')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['high', 'low', 'medium'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the order is not correct (low corresponds to the label 1, but in the csv they expect the low to be the third column), we must swap columns for y_pred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be a \"bug\" below, I don't think I actually swapped the columns yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission_csv(y_pred, X_test_indexes):\n",
    "    df = pd.DataFrame(y_pred, columns=le.classes_)\n",
    "    df.index = X_test_indexes\n",
    "    df.index.name = 'listing_id'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = create_submission_csv(y_pred, X_test_indexes)\n",
    "output.to_csv('decision_tree_predictions_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score from Kaggle\n",
    "9.18820 <- ignore, this is the old score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74659, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 2\n",
    "\n",
    "It seems that using all the features is not very good. Let's use a smaller set of features.\n",
    "We can choose the top x features in order of descending mutual information (wrt the label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feats = [10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000, 35520]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_info = mutual_info_classif(X_train, y_train, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean log loss test  score using top 50 features: 11.9486\n",
      "Mean log loss train score using top 50 features: 0.0013\n"
     ]
    }
   ],
   "source": [
    "# n_feat = 50\n",
    "# top_features = mutual_info.argsort()[-1 * n_feat:][::-1]\n",
    "\n",
    "# # Only use these top n features when training, to eliminate overfitting.\n",
    "# X_train_final = X_train[:,top_features]\n",
    "\n",
    "# model = tree.DecisionTreeClassifier()\n",
    "# scores = cross_validate(model, X_train_final, y_train, scoring='neg_log_loss', cv=5, return_train_score=True)\n",
    "# # Convert negative log loss to log loss\n",
    "# test_scores = -1 * scores['test_score']\n",
    "# train_scores = -1 * scores['train_score']\n",
    "# score = test_scores.mean().round(4)\n",
    "# train_score = train_scores.mean().round(4)\n",
    "# print(\"Mean log loss test  score using top {0} features: {1}\".format(n_feat, score))\n",
    "# print(\"Mean log loss train score using top {0} features: {1}\".format(n_feat, train_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considering top 10 features.\n",
      "Mean log loss score using top 10 features: 12.2594\n",
      "Mean log loss train score using top 10 features: 0.0145\n",
      "Considering top 20 features.\n",
      "Mean log loss score using top 20 features: 11.4878\n",
      "Mean log loss train score using top 20 features: 0.0135\n",
      "Considering top 50 features.\n",
      "Mean log loss score using top 50 features: 11.7795\n",
      "Mean log loss train score using top 50 features: 0.0012\n",
      "Considering top 100 features.\n",
      "Mean log loss score using top 100 features: 11.9161\n",
      "Mean log loss train score using top 100 features: 0.0012\n",
      "Considering top 200 features.\n",
      "Mean log loss score using top 200 features: 11.6987\n",
      "Mean log loss train score using top 200 features: 0.0008\n",
      "Considering top 500 features.\n",
      "Mean log loss score using top 500 features: 11.6289\n",
      "Mean log loss train score using top 500 features: 0.0008\n",
      "Considering top 1000 features.\n",
      "Mean log loss score using top 1000 features: 11.4237\n",
      "Mean log loss train score using top 1000 features: 0.0008\n",
      "Considering top 2000 features.\n",
      "Mean log loss score using top 2000 features: 11.4985\n",
      "Mean log loss train score using top 2000 features: 0.0008\n",
      "Considering top 5000 features.\n",
      "Mean log loss score using top 5000 features: 11.2345\n",
      "Mean log loss train score using top 5000 features: 0.0008\n",
      "Considering top 10000 features.\n",
      "Mean log loss score using top 10000 features: 11.2097\n",
      "Mean log loss train score using top 10000 features: 0.0002\n",
      "Considering top 35520 features.\n",
      "Mean log loss score using top 35520 features: 11.1606\n",
      "Mean log loss train score using top 35520 features: 0.0\n",
      "The best number of features is 35520\n",
      "The log loss for this number of features is 11.1606\n",
      "The training log loss for this number of features is 0.0\n"
     ]
    }
   ],
   "source": [
    "best_score = 9999999999999999\n",
    "best_train_score = 9999999999999\n",
    "best_n = 0\n",
    "\n",
    "for n_feat in n_feats:\n",
    "    print(\"Considering top {0} features.\".format(n_feat))\n",
    "    top_features = mutual_info.argsort()[-1 * n_feat:][::-1]\n",
    "\n",
    "    # Only use these top n features when training, to eliminate overfitting.\n",
    "    X_train_final = X_train[:,top_features]\n",
    "    \n",
    "    model = tree.DecisionTreeClassifier()\n",
    "    scores = cross_validate(model, X_train_final, y_train, scoring='neg_log_loss', cv=5, return_train_score=True)\n",
    "    # Convert negative log loss to log loss\n",
    "    test_scores = -1 * scores['test_score']\n",
    "    train_scores = -1 * scores['train_score']\n",
    "\n",
    "    score = test_scores.mean().round(4)\n",
    "    train_score = train_scores.mean().round(4)\n",
    "    \n",
    "    cross_val_scores[n_feat] = {\n",
    "        'train': train_score,\n",
    "        'test': score\n",
    "    }\n",
    "    \n",
    "    print(\"Mean log loss score using top {0} features: {1}\".format(n_feat, score))\n",
    "    print(\"Mean log loss train score using top {0} features: {1}\".format(n_feat, train_score))\n",
    "    \n",
    "    cross_val_scores[n_feat] = score\n",
    "    \n",
    "    if score < best_score:\n",
    "        best_score = score\n",
    "        best_train_score = train_score\n",
    "        best_n = n_feat\n",
    "\n",
    "print(\"The best number of features is {0}\".format(best_n))\n",
    "print(\"The log loss for this number of features is {0}\".format(best_score))\n",
    "print(\"The training log loss for this number of features is {0}\".format(best_train_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores_for_n = [ 0.0145, 0.0135, 0.0012, 0.0012, 0.0008, 0.0008, 0.0008, 0.0008, 0.0008, 0.0002, 0]\n",
    "test_scores_for_n = [12.2594, 11.4878, 11.7795, 11.9161, 11.6987, 11.6289, 11.4237, 11.4985, 11.2345, 11.2097, 11.1606 ]\n",
    "x_range = [10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000, 35520]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_range, train_scores_for_n, 'g', label='train')\n",
    "plt.plot(x_range, test_scores_for_n, 'b', label='validation')\n",
    "plt.legend()\n",
    "plt.xlabel('log(number of features)')\n",
    "plt.ylabel('log loss')\n",
    "plt.xscale('log')\n",
    "plt.title('Mean train/validation loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on entire dataset\n",
    "X_train_final = X_test[:,best_n]\n",
    "X_test_final = X_test[:,best_n]\n",
    "\n",
    "model = tree.DecisionTreeClassifier(criterion='gini')\n",
    "model.fit(X_train_final, y_train)\n",
    "\n",
    "y_pred = model.predict_proba(X_test_final)\n",
    "\n",
    "output = create_submission_csv( y_pred, X_test_indexes)\n",
    "output.to_csv('decision_tree_predictions_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 3\n",
    "Clearly something is wrong with using the 35,000+ features, which includes the following:\n",
    "* Description word counts\n",
    "* Feature word counts\n",
    "* One hot encodings for building id, manager id\n",
    "* Year, month, day, etc.\n",
    "\n",
    "One thing we can try is reduce the number of features to choose from. We can try excluding the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(train_df, min_feats=5):\n",
    "    bathrooms = train_df['bathrooms']\n",
    "    bedrooms = train_df['bedrooms']\n",
    "#     building_ids = train_df['building_id']\n",
    "    latitudes = train_df['latitude']\n",
    "    longitudes = train_df['longitude']\n",
    "#     manager_ids = train_df['manager_id']\n",
    "    prices = train_df['price']\n",
    "    \n",
    "    datetime = pd.to_datetime(train_df['created'])\n",
    "    \n",
    "    months = datetime.dt.month\n",
    "    days = datetime.dt.day\n",
    "    hours = datetime.dt.hour\n",
    "    \n",
    "    # Where Monday = 0, and Sunday = 6\n",
    "    weekdays = datetime.dt.dayofweek\n",
    "    \n",
    "    num_photos = train_df['photos'].str.len()\n",
    "    \n",
    "    # The final dataframe to be returned\n",
    "    final_train_df = pd.DataFrame()\n",
    "    \n",
    "    final_train_df['bathrooms'] = bathrooms\n",
    "    final_train_df['bedrooms'] = bedrooms\n",
    "#     final_train_df['building_ids'] = building_ids \n",
    "    final_train_df['latitudes'] = latitudes\n",
    "    final_train_df['longitudes'] = longitudes\n",
    "#     final_train_df['manager_ids'] = manager_ids\n",
    "    final_train_df['prices'] = prices\n",
    "    \n",
    "    final_train_df['months'] = months\n",
    "    final_train_df['days'] = days\n",
    "    final_train_df['hours'] = hours\n",
    "    \n",
    "    final_train_df['weekdays'] = weekdays\n",
    "    final_train_df['num_photos'] = num_photos\n",
    "    final_train_df.index = train_df['listing_id']\n",
    "\n",
    "    return final_train_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = train_df.drop(columns=['interest_level'])\n",
    "X_test2 = test_df\n",
    "X_train2 = preprocess(X_train2)\n",
    "X_test2 = preprocess(X_test2)\n",
    "drop_cols = ['num_photos', 'months', 'days', 'hours', 'weekdays']\n",
    "X_train2 = X_train2.drop(columns=drop_cols)\n",
    "X_test2 = X_test2.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of features is rather small, but at least it is clean. Let's try cross-validation again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean log loss test  score: 9.667\n",
      "Mean log loss train score: 0.1439\n"
     ]
    }
   ],
   "source": [
    "model = tree.DecisionTreeClassifier()\n",
    "scores = cross_validate(model, X_train2, y_train, scoring='neg_log_loss', cv=5, return_train_score=True)\n",
    "\n",
    "# Convert negative log loss to log loss\n",
    "test_scores = -1 * scores['test_score']\n",
    "train_scores = -1 * scores['train_score']\n",
    "score = test_scores.mean().round(4)\n",
    "train_score = train_scores.mean().round(4)\n",
    "\n",
    "print(\"Mean log loss test  score: {0}\".format(score))\n",
    "print(\"Mean log loss train score: {0}\".format( train_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on entire dataset\n",
    "model = tree.DecisionTreeClassifier(criterion='gini')\n",
    "model.fit(X_train2, y_train)\n",
    "\n",
    "y_pred = model.predict_proba(X_test2)\n",
    "\n",
    "output = create_submission_csv(y_pred, X_test_indexes)\n",
    "output.to_csv('decision_tree_predictions_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 4\n",
    "\n",
    "If reducing the number of features is not working out, then we should find other ways to reduce overfitting. Pruning is known to be a well-known method for doing so.\n",
    "\n",
    "- entropy for information gain\n",
    "- limiting max_depth to like 30 or so\n",
    "- class_weight = balanced\n",
    "- ccp_alpha = ??? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00, -9.03501810e-21, -6.77626358e-21, ...,\n",
       "        5.23884245e-03,  5.86846096e-03,  1.32798860e-02])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train on entire dataset\n",
    "model = tree.DecisionTreeClassifier(criterion='gini')\n",
    "model.fit(X_train2, y_train)\n",
    "path = model.cost_complexity_pruning_path(X_train2, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "ccp_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013279886031082988"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(ccp_alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search for optimal parameters\n",
    "\n",
    "Because of overfitting, we will use a relatively small number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'ccp_alpha': [1e-10, 1e-8, 1e-6, 1e-4, 1e-2, 1e-0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier()\n",
    "clf = GridSearchCV(model, parameters, scoring='neg_log_loss', cv=5, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter ccp_alpha for estimator DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n                       max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, presort=False,\n                       random_state=None, splitter='best'). Check the list of available parameters with `estimator.get_params().keys()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-fb4008bef9bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    686\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1147\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1149\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    665\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 667\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    919\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[0mtrain_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mset_params\u001b[1;34m(self, **params)\u001b[0m\n\u001b[0;32m    222\u001b[0m                                  \u001b[1;34m'Check the list of available parameters '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m                                  \u001b[1;34m'with `estimator.get_params().keys()`.'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m                                  (key, self))\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdelim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid parameter ccp_alpha for estimator DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n                       max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, presort=False,\n                       random_state=None, splitter='best'). Check the list of available parameters with `estimator.get_params().keys()`."
     ]
    }
   ],
   "source": [
    "result = clf.fit(X_train2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features=None,\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              presort='deprecated',\n",
       "                                              random_state=None,\n",
       "                                              splitter='best'),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'ccp_alpha': [1e-10, 1e-08, 1e-06, 0.0001, 0.01, 1.0],\n",
       "                         'class_weight': [None, 'balanced'],\n",
       "                         'criterion': ['gini', 'entropy'],\n",
       "                         'min_samples_leaf': [1, 2, 3],\n",
       "                         'splitter': ['best', 'random']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring='neg_log_loss', verbose=0)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ccp_alpha': 0.0001,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'min_samples_leaf': 1,\n",
       " 'splitter': 'best'}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7071378632561878"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-10.93096494, -11.01497057,  -8.23351146,  -6.60870272,\n",
       "        -6.48554245,  -4.47055091, -10.8981956 , -11.09059213,\n",
       "        -8.43868869,  -6.66221713,  -6.56497346,  -4.40287725,\n",
       "       -10.86231038, -11.09500554,  -8.19771318,  -6.60693549,\n",
       "        -6.32698222,  -4.49447195, -10.87787802, -11.03222916,\n",
       "        -8.43636246,  -6.63343241,  -6.51069618,  -4.50861596,\n",
       "       -10.91117473, -11.06000583,  -8.26901721,  -6.60050439,\n",
       "        -6.44929852,  -4.37457586, -10.8664666 , -11.05011638,\n",
       "        -8.42288085,  -6.57829861,  -6.58823577,  -4.43674298,\n",
       "       -10.86963002, -10.94969721,  -8.21022264,  -6.61941576,\n",
       "        -6.32649461,  -4.44401239, -10.8817089 , -11.03485412,\n",
       "        -8.46307988,  -6.66704472,  -6.50040007,  -4.52790285,\n",
       "       -10.90011216, -11.05470673,  -8.25930225,  -6.62615696,\n",
       "        -6.47050781,  -4.33358343, -10.90297845, -11.12776446,\n",
       "        -8.45078019,  -6.55836522,  -6.54996847,  -4.36474493,\n",
       "       -10.86620585, -11.12616466,  -8.22041202,  -6.58502451,\n",
       "        -6.30774085,  -4.50547423, -10.90597587, -11.06833439,\n",
       "        -8.45397689,  -6.67118045,  -6.50778802,  -4.59111838,\n",
       "        -0.70713786,  -0.72981119,  -0.70713786,  -0.71361521,\n",
       "        -0.70713786,  -0.7295097 ,  -3.9213467 ,  -1.63244166,\n",
       "        -3.69293394,  -1.62576706,  -3.31121562,  -1.3926023 ,\n",
       "        -1.65570626,  -1.12045142,  -1.6242696 ,  -1.12636101,\n",
       "        -1.57174015,  -1.11056143,  -4.86867379,  -4.14888066,\n",
       "        -4.49063745,  -3.5044153 ,  -3.98082775,  -2.81799598,\n",
       "        -0.76442994,  -0.78875263,  -0.76442994,  -0.78875263,\n",
       "        -0.76442994,  -0.78875263,  -0.72482352,  -0.78875263,\n",
       "        -0.72482352,  -0.78875263,  -0.72482352,  -0.78875263,\n",
       "        -1.02289819,  -1.09861229,  -1.02289819,  -1.09201408,\n",
       "        -1.02289819,  -1.09861229,  -0.96731094,  -1.08968783,\n",
       "        -0.96731094,  -1.09607727,  -0.96731094,  -1.07885516,\n",
       "        -0.78875263,  -0.78875263,  -0.78875263,  -0.78875263,\n",
       "        -0.78875263,  -0.78875263,  -0.78875263,  -0.78875263,\n",
       "        -0.78875263,  -0.78875263,  -0.78875263,  -0.78875263,\n",
       "        -1.09861229,  -1.09861229,  -1.09861229,  -1.09861229,\n",
       "        -1.09861229,  -1.09861229,  -1.09861229,  -1.09861229,\n",
       "        -1.09861229,  -1.09861229,  -1.09861229,  -1.09861229])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean log loss test  score  9.6809\n",
      "Mean log loss train score  0.1439\n"
     ]
    }
   ],
   "source": [
    "model = tree.DecisionTreeClassifier(criterion='gini', ccp_alpha=0, class_weight=None,\n",
    "                                   min_samples_leaf=1, splitter='best')\n",
    "scores = cross_validate(model, X_train2, y_train, scoring='neg_log_loss', cv=5, return_train_score=True)\n",
    "\n",
    "# Convert negative log loss to log loss\n",
    "test_scores = -1 * scores['test_score']\n",
    "train_scores = -1 * scores['train_score']\n",
    "score = test_scores.mean().round(4)\n",
    "train_score = train_scores.mean().round(4)\n",
    "print(\"Mean log loss test  score  {0}\".format(score))\n",
    "print(\"Mean log loss train score  {0}\".format(train_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on entire dataset\n",
    "\n",
    "model = tree.DecisionTreeClassifier(criterion='gini', ccp_alpha=0, class_weight=None,\n",
    "                                   min_samples_leaf=1, splitter='best')\n",
    "model.fit(X_train2, y_train)\n",
    "\n",
    "y_pred = model.predict_proba(X_test2)\n",
    "\n",
    "output = create_submission_csv(y_pred, X_test_indexes)\n",
    "output.to_csv('decision_tree_predictions_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on entire dataset\n",
    "\n",
    "model = tree.DecisionTreeClassifier(criterion='gini', ccp_alpha=0.0001, class_weight=None,\n",
    "                                   min_samples_leaf=1, splitter='best')\n",
    "model.fit(X_train2, y_train)\n",
    "\n",
    "y_pred = model.predict_proba(X_test2)\n",
    "\n",
    "output = create_submission_csv(y_pred, X_test_indexes)\n",
    "output.to_csv('decision_tree_predictions_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
